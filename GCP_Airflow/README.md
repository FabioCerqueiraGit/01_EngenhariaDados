# Pipeline de ETL com Apache Airflow no Google Cloud Platform (GCP) ðŸš€

## VisÃ£o Geral
Este projeto implementa um pipeline de ETL utilizando o Apache Airflow para orquestraÃ§Ã£o no Google Cloud Platform (GCP). O objetivo Ã© extrair dados de uma API de e-commerce, processÃ¡-los com Apache Beam no Dataflow e armazenÃ¡-los no BigQuery para anÃ¡lises futuras.

## Arquitetura do Pipeline
1. **ExtraÃ§Ã£o**: Coleta de dados de vendas de uma API REST e armazenamento temporÃ¡rio no Google Cloud Storage (GCS).
2. **TransformaÃ§Ã£o**: Processamento dos dados utilizando Apache Beam e execuÃ§Ã£o no Dataflow.
3. **Carga**: Armazenamento dos dados processados no BigQuery para anÃ¡lise.
4. **OrquestraÃ§Ã£o**: O Apache Airflow gerencia todas as etapas do pipeline.

## Tecnologias Utilizadas
- **Google Cloud Platform (GCP)**
- **Apache Airflow** para orquestraÃ§Ã£o
- **Google Cloud Storage (GCS)** para armazenamento de dados brutos
- **Apache Beam & Dataflow** para transformaÃ§Ã£o de dados
- **BigQuery** para armazenamento e anÃ¡lise dos dados
- **Cloud Composer** para execuÃ§Ã£o gerenciada do Airflow

## ConfiguraÃ§Ã£o do Ambiente
### PrÃ©-requisitos
1. Conta ativa no Google Cloud Platform (GCP)
2. Projeto configurado no GCP com faturamento habilitado
3. APIs do Cloud Storage, Dataflow, BigQuery e Cloud Composer ativadas
4. Ambiente do Cloud Composer configurado com Apache Airflow

### Passos de ConfiguraÃ§Ã£o
1. Criar um bucket no Google Cloud Storage:
   ```sh
   gsutil mb gs://meu-bucket-de-dados
   ```
2. Criar um dataset no BigQuery:
   ```sql
   CREATE DATASET ecommerce_sales;
   ```
3. Configurar o ambiente do Cloud Composer pelo console do GCP.

## Estrutura do RepositÃ³rio
```
ðŸ“‚ GCP_Airflow
 â”£ ðŸ“‚ dags
 â”ƒ â”£ ðŸ“œ etl_pipeline.py  # DAG do Apache Airflow
 â”£ ðŸ“‚ data
 â”ƒ â”£ ðŸ“œ sample_data.json  # Dados de exemplo
 â”£ ðŸ“‚ scripts
 â”ƒ â”£ ðŸ“œ transform.py  # CÃ³digo para Apache Beam
 â”£ ðŸ“œ README.md
```

## ExecuÃ§Ã£o do Pipeline
1. Subir a DAG do Airflow no Cloud Composer.
2. A DAG extrai os dados da API e os armazena no GCS.
3. O Apache Beam processa os dados e os salva no BigQuery.
4. O pipeline pode ser monitorado pelo Airflow.

## PrÃ³ximos Passos
- Melhorar a eficiÃªncia do pipeline com otimizaÃ§Ãµes no Dataflow.
- Implementar monitoramento detalhado com Stackdriver.
- Adicionar testes automatizados para validaÃ§Ã£o de dados.

## ContribuiÃ§Ãµes
ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues e pull requests. ðŸš€

